{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "feedforward",
      "language": "python",
      "name": "feedforward"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Week 2 Programming Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QlTlUeeroFWo",
        "zKPz3vv-oFWy",
        "fNVmkwyToFW8",
        "e1DORpGooFXT",
        "mH6wmfNAoFXc"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jABhOvG_oFV5"
      },
      "source": [
        "# Programming Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOfScRJ_oFV6"
      },
      "source": [
        "## Bayesian convolutional neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kTxcfwxoFV6"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In this notebook, you will create a Bayesian convolutional neural network to classify the famous MNIST handwritten digits. This will be a probabilistic model, designed to capture both aleatoric and epistemic uncertainty. You will test the uncertainty quantifications against a corrupted version of the dataset.\n",
        "\n",
        "Some code cells are provided for you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
        "\n",
        "`#### GRADED CELL ####`\n",
        "\n",
        "Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly.\n",
        "\n",
        "### How to submit\n",
        "\n",
        "Complete all the tasks you are asked for in the worksheet. When you have finished and are happy with your code, press the **Submit Assignment** button at the top of this notebook.\n",
        "\n",
        "### Let's get started!\n",
        "\n",
        "We'll start running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdB-aS_aoFV7"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "\n",
        "# If you would like to make further imports from tensorflow, add them here\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLoEIz_5oFV-"
      },
      "source": [
        "#### The MNIST and MNIST-C datasets\n",
        "\n",
        "In this assignment, you will use the [MNIST](http://yann.lecun.com/exdb/mnist/) and [MNIST-C](https://github.com/google-research/mnist-c) datasets, which both consist of a training set of 60,000 handwritten digits with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The MNIST-C dataset is a corrupted version of the MNIST dataset, to test out-of-distribution robustness of computer vision models.\n",
        "\n",
        "- Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n",
        "- N. Mu and J. Gilmeer. \"MNIST-C: A Robustness Benchmark for Computer Vision\" https://arxiv.org/abs/1906.02337\n",
        "\n",
        "Your goal is to construct a neural network that classifies images of handwritten digits into one of 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67o1JQ9yqWuv"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The datasets required for this project can be downloaded from the following links:\n",
        "\n",
        "https://drive.google.com/file/d/10VhBL5zo4cOA_28trFCu3WtxFBHbj3yV/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/11013-Bk-iJjVZ1rPn1TFPut12WNhMu5q/view?usp=sharing\n",
        "\n",
        "You should store these files in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn-dqkOBqxxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e04f13-1cc4-40f1-c0af-227f638e942a"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ4MeGuki6NA",
        "outputId": "7af24738-6b00-4aa1-f37a-274413c926db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_path = './gdrive/MyDrive/Colab Notebooks/data/'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 22M\n",
            "-rw------- 1 root root  22M Jan  2 17:12 'CelebA subset.zip'\n",
            "drwx------ 2 root root 4.0K Jan  4 13:15  MNIST\n",
            "drwx------ 2 root root 4.0K Jan  4 13:15  MNIST_corrupted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdq_bGwyoFV-"
      },
      "source": [
        "#### Load the datasets\n",
        "\n",
        "We'll start by importing two datasets. The first is the MNIST dataset of handwritten digits, and the second is the MNIST-C dataset, which is a corrupted version of the MNIST dataset. This dataset is available on [TensorFlow datasets](https://www.tensorflow.org/datasets/catalog/mnist_corrupted). We'll be using the dataset with \"spatters\". We will load and inspect the datasets below. We'll use the notation `_c` to denote `corrupted`. The images are the same as in the original MNIST, but are \"corrupted\" by some grey spatters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoZrjcjsoFV_"
      },
      "source": [
        "# Function to load training and testing data, with labels in integer and one-hot form\n",
        "\n",
        "def load_data(name):\n",
        "    data_dir = os.path.join(data_path, name)\n",
        "    x_train = 1 - np.load(os.path.join(data_dir, 'x_train.npy')) / 255.\n",
        "    x_train = x_train.astype(np.float32)\n",
        "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
        "    y_train_oh = tf.keras.utils.to_categorical(y_train)\n",
        "    x_test  = 1 - np.load(os.path.join(data_dir, 'x_test.npy')) / 255.\n",
        "    x_test = x_test.astype(np.float32)\n",
        "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
        "    y_test_oh = tf.keras.utils.to_categorical(y_test)\n",
        "    \n",
        "    return (x_train, y_train, y_train_oh), (x_test, y_test, y_test_oh)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICc0b73GoFWB"
      },
      "source": [
        "# Function to inspect dataset digits\n",
        "\n",
        "def inspect_images(data, num_images):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=num_images, figsize=(2*num_images, 2))\n",
        "    for i in range(num_images):\n",
        "        ax[i].imshow(data[i, ..., 0], cmap='gray')\n",
        "        ax[i].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q_akHFvoFWF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "b0290930-7cdb-478e-89f3-c6d9de18661f"
      },
      "source": [
        "# Load and inspect the MNIST dataset\n",
        "\n",
        "(x_train, y_train, y_train_oh), (x_test, y_test, y_test_oh) = load_data('MNIST')\n",
        "inspect_images(data=x_train, num_images=8)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x144 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER5aLNPnoFWI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "b37b6093-cc21-48fa-a428-35040cc18155"
      },
      "source": [
        "# Load and inspect the MNIST-C dataset\n",
        "\n",
        "(x_c_train, y_c_train, y_c_train_oh), (x_c_test, y_c_test, y_c_test_oh) = load_data('MNIST_corrupted')\n",
        "inspect_images(data=x_c_train, num_images=8)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x144 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATtylddtoFWK"
      },
      "source": [
        "#### Create the deterministic model\n",
        "\n",
        "We will first train a standard deterministic CNN classifier model as a base model before implementing the probabilistic and Bayesian neural networks. You should now build the deterministic model using the Sequential API according to the following specifications:\n",
        "\n",
        "* The first layer should be Conv2D layer with 8 filters, 5x5 kernel size, ReLU activation and `'VALID'` padding.\n",
        "    * This layer should set the `input_shape` according to the function argument\n",
        "* The second layer should be a MaxPooling2D layer with a 6x6 window size.\n",
        "* The third layer should be a Flatten layer\n",
        "* The final layer should be a Dense layer with 10 units and softmax activation\n",
        "\n",
        "In total, the network should have 4 layers.\n",
        "\n",
        "The model should then be compiled with the loss function, optimiser and list of metrics supplied in the function arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwu2unFloFWK"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def get_deterministic_model(input_shape, loss, optimizer, metrics):\n",
        "    \"\"\"\n",
        "    This function should build and compile a CNN model according to the above specification. \n",
        "    The function takes input_shape, loss, optimizer and metrics as arguments, which should be\n",
        "    used to define and compile the model.\n",
        "    Your function should return the compiled model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(8, (5,5), activation='relu', padding='VALID', input_shape=input_shape),\n",
        "        MaxPooling2D((6,6)),\n",
        "        Flatten(),\n",
        "        Dense(10, activation='softmax'),\n",
        "    ])\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "    return model    \n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MPJousoFWN"
      },
      "source": [
        "# Run your function to get the benchmark model\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "deterministic_model = get_deterministic_model(\n",
        "    input_shape=(28, 28, 1), \n",
        "    loss=SparseCategoricalCrossentropy(), \n",
        "    optimizer=RMSprop(), \n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHH61DKIoFWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a164861f-839a-4325-9bcf-0fed1ea2c941"
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "deterministic_model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 8)         208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,498\n",
            "Trainable params: 1,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKQwGmx4oFWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c244664-75ff-4656-ab58-7266ed4eb32f"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "deterministic_model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 10s 2ms/step - loss: 0.9634 - accuracy: 0.7324\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1604 - accuracy: 0.9525\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1228 - accuracy: 0.9629\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1066 - accuracy: 0.9672\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0959 - accuracy: 0.9710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa01b4c25c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH4evy33oFWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02389b7a-ef37-4f03-acce-49cb3d0f22a6"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "print('Accuracy on MNIST test set: ',\n",
        "      str(deterministic_model.evaluate(x_test, y_test, verbose=False)[1]))\n",
        "print('Accuracy on corrupted MNIST test set: ',\n",
        "      str(deterministic_model.evaluate(x_c_test, y_c_test, verbose=False)[1]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on MNIST test set:  0.9732999801635742\n",
            "Accuracy on corrupted MNIST test set:  0.9413999915122986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gL4XmbLoFWW"
      },
      "source": [
        "As you might expect, the pointwise performance on the corrupted MNIST set is worse. This makes sense, since this dataset is slightly different, and noisier, than the uncorrupted version. Furthermore, the model was trained on the uncorrupted MNIST data, so has no experience with the spatters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgADj27goFWX"
      },
      "source": [
        "### Probabilistic CNN model\n",
        "\n",
        "You'll start by turning this deterministic network into a probabilistic one, by letting the model output a distribution instead of a deterministic tensor. This model will capture the aleatoric uncertainty on the image labels. You will do this by adding a probabilistic layer to the end of the model and training using the negative loglikelihood. \n",
        "\n",
        "You should first define the negative loss likelihood loss function below. This function has arguments `y_true` for the correct label (as a one-hot vector), and `y_pred` as the model prediction (a `OneHotCategorical` distribution). It should return the negative log-likelihood of each sample in `y_true` given the predicted distribution `y_pred`. If `y_true` is of shape `[B, E]` and `y_pred` has batch shape `[B]` and event shape `[E]`, the output should be a Tensor of shape `[B]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBLyvWOtoFWX"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following functions. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def nll(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    This function should return the negative log-likelihood of each sample\n",
        "    in y_true given the predicted distribution y_pred. If y_true is of shape \n",
        "    [B, E] and y_pred has batch shape [B] and event_shape [E], the output \n",
        "    should be a Tensor of shape [B].\n",
        "    \"\"\"\n",
        "    return - y_pred.log_prob(y_true)    \n",
        "    "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtXrZEWpoFWZ"
      },
      "source": [
        "You should now build your probabilistic model according to the following specification:\n",
        "\n",
        "* The first three layers are the same as for the deterministic model above\n",
        "* The fourth layer should be a Dense layer with no activation function, and the correct number of units needed to parameterise the probabilistic layer that follows\n",
        "* The final layer should be a probabilistic layer that outputs a `OneHotCategorical` distribution with an event shape of `[10]`, corresponding to the 10 digits\n",
        "* The `convert_to_tensor_fn` in the categorical layer should be set to the mode\n",
        "\n",
        "In total, your model should have 5 layers.\n",
        "\n",
        "The model should then be compiled with the loss function, optimiser and list of metrics supplied in the function arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aRtkNN_loFWa"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following functions. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def get_probabilistic_model(input_shape, loss, optimizer, metrics):\n",
        "    \"\"\"\n",
        "    This function should return the probabilistic model according to the \n",
        "    above specification.\n",
        "    The function takes input_shape, loss, optimizer and metrics as arguments, which should be\n",
        "    used to define and compile the model.\n",
        "    Your function should return the compiled model.\n",
        "    \"\"\"\n",
        "    event_size = 10\n",
        "    model = Sequential([\n",
        "        Conv2D(8, (5,5), activation='relu', padding='VALID', input_shape=input_shape),\n",
        "        MaxPooling2D((6,6)),\n",
        "        Flatten(),\n",
        "        #Dense(units=tfpl.OneHotCategorical.params_size(event_size)),\n",
        "        Dense(units=10),\n",
        "        tfpl.OneHotCategorical(event_size=event_size,\n",
        "                               convert_to_tensor_fn=tfd.Distribution.mode)\n",
        "        \n",
        "    ])\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "    return model        \n",
        "    "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCLuN7lpoFWc"
      },
      "source": [
        "# Run your function to get the probabilistic model\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "probabilistic_model = get_probabilistic_model(\n",
        "    input_shape=(28, 28, 1), \n",
        "    loss=nll, \n",
        "    optimizer=RMSprop(), \n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WITIcnsyoFWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6ceb31-bb66-4a67-e743-418d67337dc3"
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "probabilistic_model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 8)         208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "one_hot_categorical_2 (OneHo multiple                  0         \n",
            "=================================================================\n",
            "Total params: 1,498\n",
            "Trainable params: 1,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN6VkX4-oFWg"
      },
      "source": [
        "Now, you can train the probabilistic model on the MNIST data using the code below. \n",
        "\n",
        "Note that the target data now uses the one-hot version of the labels, instead of the sparse version. This is to match the categorical distribution you added at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAW94TLLoFWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6bd014f1-97f8-4f6e-bd23-edcc56942aa2"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "probabilistic_model.fit(x_train, y_train_oh, epochs=5)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-18ac26ba3303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprobabilistic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:154 __call__\n        losses, sample_weight, reduction=self._get_reduction())\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:256 compute_weighted_loss\n        losses = ops.convert_to_tensor_v2_with_dispatch(losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1405 convert_to_tensor_v2_with_dispatch\n        value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1415 convert_to_tensor_v2\n        as_ref=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:265 constant\n        allow_broadcast=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:283 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto\n        raise ValueError(\"None values not supported.\")\n\n    ValueError: None values not supported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vczPrd_8oFWj"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "print('Accuracy on MNIST test set: ',\n",
        "      str(probabilistic_model.evaluate(x_test, y_test_oh, verbose=False)[1]))\n",
        "print('Accuracy on corrupted MNIST test set: ',\n",
        "      str(probabilistic_model.evaluate(x_c_test, y_c_test_oh, verbose=False)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMILWQefoFWl"
      },
      "source": [
        "Note that the test accuracy of the probabilistic model is identical to the deterministic model. This is because the model architectures for both are equivalent; the only difference being that the probabilistic model returns a distribution object. Since we have also set the same random seed for both models, the trained variables are in fact identical, as the following cell shows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiznxCoVoFWl"
      },
      "source": [
        "# Check all the weights of the deterministic and probabilistic models are identical\n",
        "\n",
        "for deterministic_variable, probabilistic_variable in zip(deterministic_model.weights, probabilistic_model.weights):\n",
        "    print(np.allclose(deterministic_variable.numpy(), probabilistic_variable.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlTlUeeroFWo"
      },
      "source": [
        "#### Analyse the model predictions\n",
        "\n",
        "We will now do some deeper analysis by looking at the probabilities the model assigns to each class instead of its single prediction. \n",
        "\n",
        "The function below will be useful to help us analyse the probabilistic model predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOeY0WmboFWp"
      },
      "source": [
        "# Function to make plots of the probabilities that the model estimates for an image\n",
        "\n",
        "def analyse_model_prediction(data, true_labels, model, image_num, run_ensemble=False):\n",
        "    if run_ensemble:\n",
        "        ensemble_size = 200\n",
        "    else:\n",
        "        ensemble_size = 1\n",
        "    image = data[image_num]\n",
        "    true_label = true_labels[image_num, 0]\n",
        "    predicted_probabilities = np.empty(shape=(ensemble_size, 10))\n",
        "    for i in range(ensemble_size):\n",
        "        predicted_probabilities[i] = model(image[np.newaxis, :]).mean().numpy()[0]\n",
        "    model_prediction = model(image[np.newaxis, :])\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 2),\n",
        "                                   gridspec_kw={'width_ratios': [2, 4]})\n",
        "    \n",
        "    # Show the image and the true label\n",
        "    ax1.imshow(image[..., 0], cmap='gray')\n",
        "    ax1.axis('off')\n",
        "    ax1.set_title('True label: {}'.format(str(true_label)))\n",
        "    \n",
        "    # Show a 95% prediction interval of model predicted probabilities\n",
        "    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(10)])\n",
        "    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 97.5) for i in range(10)])    \n",
        "    bar = ax2.bar(np.arange(10), pct_97p5, color='red')\n",
        "    bar[int(true_label)].set_color('green')\n",
        "    ax2.bar(np.arange(10), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n",
        "    ax2.set_xticks(np.arange(10))\n",
        "    ax2.set_ylim([0, 1])\n",
        "    ax2.set_ylabel('Probability')\n",
        "    ax2.set_title('Model estimated probabilities')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3c8vqn9PoFWq"
      },
      "source": [
        "# Prediction examples on MNIST\n",
        "\n",
        "for i in [0, 1577]:\n",
        "    analyse_model_prediction(x_test, y_test, probabilistic_model, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHdUTb7UoFWt"
      },
      "source": [
        "The model is very confident that the first image is a 6, which is correct. For the second image, the model struggles, assigning nonzero probabilities to many different classes. \n",
        "\n",
        "Run the code below to do the same for 2 images from the corrupted MNIST test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9--Bv_ZpoFWt"
      },
      "source": [
        "# Prediction examples on MNIST-C\n",
        "\n",
        "for i in [0, 3710]:\n",
        "    analyse_model_prediction(x_c_test, y_c_test, probabilistic_model, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li3vTiOYoFWv"
      },
      "source": [
        "The first is the same 6 as you saw above, but the second image is different. Notice how the model can still say with high certainty that the first image is a 6, but struggles for the second, assigning an almost uniform distribution to all possible labels.\n",
        "\n",
        "Finally, have a look at an image for which the model is very sure on MNIST data but very unsure on corrupted MNIST data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD9Qj3izoFWw"
      },
      "source": [
        "# Prediction examples from both datasets\n",
        "\n",
        "for i in [9241]:\n",
        "    analyse_model_prediction(x_test, y_test, probabilistic_model, i)\n",
        "    analyse_model_prediction(x_c_test, y_c_test, probabilistic_model, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6OLDYt-oFWx"
      },
      "source": [
        "It's not surprising what's happening here: the spatters cover up most of the number. You would hope a model indicates that it's unsure here, since there's very little information to go by. This is exactly what's happened."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPz3vv-oFWy"
      },
      "source": [
        "#### Uncertainty quantification using entropy\n",
        "\n",
        "We can also make some analysis of the model's uncertainty across the full test set, instead of for individual values. One way to do this is to calculate the [entropy](https://en.wikipedia.org/wiki/Entropy_%28information_theory%29) of the distribution. The entropy is the expected information (or informally, the expected 'surprise') of a random variable, and is a measure of the uncertainty of the random variable. The entropy of the estimated probabilities for sample $i$ is defined as\n",
        "\n",
        "$$\n",
        "H_i = -\\sum_{j=1}^{10} p_{ij} \\text{log}_{2}(p_{ij})\n",
        "$$\n",
        "\n",
        "where $p_{ij}$ is the probability that the model assigns to sample $i$ corresponding to label $j$. The entropy as above is measured in _bits_. If the natural logarithm is used instead, the entropy is measured in _nats_.\n",
        "\n",
        "The key point is that the higher the value, the more unsure the model is. Let's see the distribution of the entropy of the model's predictions across the MNIST and corrupted MNIST test sets. The plots will be split between predictions the model gets correct and incorrect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Ev31u7oFWy"
      },
      "source": [
        "# Functions to plot the distribution of the information entropy across samples,\n",
        "# split into whether the model prediction is correct or incorrect\n",
        "\n",
        "\n",
        "def get_correct_indices(model, x, labels):\n",
        "    y_model = model(x)\n",
        "    correct = np.argmax(y_model.mean(), axis=1) == np.squeeze(labels)\n",
        "    correct_indices = [i for i in range(x.shape[0]) if correct[i]]\n",
        "    incorrect_indices = [i for i in range(x.shape[0]) if not correct[i]]\n",
        "    return correct_indices, incorrect_indices\n",
        "\n",
        "\n",
        "def plot_entropy_distribution(model, x, labels):\n",
        "    probs = model(x).mean().numpy()\n",
        "    entropy = -np.sum(probs * np.log2(probs), axis=1)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    for i, category in zip(range(2), ['Correct', 'Incorrect']):\n",
        "        entropy_category = entropy[get_correct_indices(model, x, labels)[i]]\n",
        "        mean_entropy = np.mean(entropy_category)\n",
        "        num_samples = entropy_category.shape[0]\n",
        "        title = category + 'ly labelled ({:.1f}% of total)'.format(num_samples / x.shape[0] * 100)\n",
        "        axes[i].hist(entropy_category, weights=(1/num_samples)*np.ones(num_samples))\n",
        "        axes[i].annotate('Mean: {:.3f} bits'.format(mean_entropy), (0.4, 0.9), ha='center')\n",
        "        axes[i].set_xlabel('Entropy (bits)')\n",
        "        axes[i].set_ylim([0, 1])\n",
        "        axes[i].set_ylabel('Probability')\n",
        "        axes[i].set_title(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dYsfnipoFW1"
      },
      "source": [
        "# Entropy plots for the MNIST dataset\n",
        "\n",
        "print('MNIST test set:')\n",
        "plot_entropy_distribution(probabilistic_model, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txpCTZEsoFW3"
      },
      "source": [
        "# Entropy plots for the MNIST-C dataset\n",
        "\n",
        "print('Corrupted MNIST test set:')\n",
        "plot_entropy_distribution(probabilistic_model, x_c_test, y_c_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h18QpMRoFW5"
      },
      "source": [
        "There are two main conclusions:\n",
        "- The model is more unsure on the predictions it got wrong: this means it \"knows\" when the prediction may be wrong.\n",
        "- The model is more unsure for the corrupted MNIST test than for the uncorrupted version. Futhermore, this is more pronounced for correct predictions than for those it labels incorrectly.\n",
        "\n",
        "In this way, the model seems to \"know\" when it is unsure. This is a great property to have in a machine learning model, and is one of the advantages of probabilistic modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTHlkzWvoFW5"
      },
      "source": [
        "### Bayesian CNN model\n",
        "\n",
        "The probabilistic model you just created considered only aleatoric uncertainty, assigning probabilities to each image instead of deterministic labels. The model still had deterministic weights. However, as you've seen, there is also 'epistemic' uncertainty over the weights, due to uncertainty about the parameters that explain the training data. \n",
        "\n",
        "You'll now be adding weight uncertainty to the model you just created. Your new model will again have the following layers:\n",
        "- 2D convolution\n",
        "- Max pooling\n",
        "- Flatten\n",
        "- Dense\n",
        "- OneHotCategorical\n",
        "\n",
        "but where the convolutional and dense layers include weight uncertainty. You'll embed weight uncertainty as follows:\n",
        "- The 2D convolution layer will be replaced by a `Convolution2DReparameterization` layer\n",
        "- The Dense layer will be replaced by a `DenseVariational` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl5EroY8oFW6"
      },
      "source": [
        "You should start by creating the convolutional layer in the function below. The function should return an instance of the `Convolution2DReparameterization` layer, according to the following specification: \n",
        "\n",
        "* The function takes the `input_shape` and `divergence_fn` as arguments\n",
        "* The layer should set the input shape in its constructor using the `input_shape` argument\n",
        "* This layer should have 8 filters, a kernel size of `(5, 5)`, a ReLU activation, and `\"VALID\"` padding\n",
        "* The prior for both the kernel and bias should be the standard `default_multivariate_normal_fn`, as seen in the coding tutorial\n",
        "* The posterior for each parameter in both the kernel and bias should be an independent normal distribution with trainable mean and variance (_hint: use the_ `default_mean_field_normal_fn`_)_\n",
        "* The divergence function should be set using the `divergence_fn` argument for both the kernel and the bias\n",
        "\n",
        "_HINT: Review the arguments you used in the coding tutorial on Reparameterization layers._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhcswiqxoFW6"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following functions. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def get_convolutional_reparameterization_layer(input_shape, divergence_fn):\n",
        "    \"\"\"\n",
        "    This function should create an instance of a Convolution2DReparameterization \n",
        "    layer according to the above specification. \n",
        "    The function takes the input_shape and divergence_fn as arguments, which should \n",
        "    be used to define the layer.\n",
        "    Your function should then return the layer instance.\n",
        "    \"\"\"\n",
        "    layer = tfpl.Convolution2DReparameterization(\n",
        "            input_shape=input_shape, filters=8, kernel_size=(5,5), activation='relu',\n",
        "            padding='VALID',\n",
        "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
        "            kernel_divergence_fn=divergence_fn,\n",
        "            bias_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
        "            bias_divergence_fn=divergence_fn,      \n",
        "        )\n",
        "    return layer\n",
        "    "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qegf6tkoFW8"
      },
      "source": [
        "You'll use this function to create your model a little bit later on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNVmkwyToFW8"
      },
      "source": [
        "#### Custom prior\n",
        "\n",
        "For the parameters of the `DenseVariational` layer, we will use a custom prior: the \"spike and slab\" (also called a *scale mixture prior*) distribution. This distribution has a density that is the weighted sum of two normally distributed ones: one with a standard deviation of 1 and one with a standard deviation of 10. In this way, it has a sharp spike around 0 (from the normal distribution with standard deviation 1), but is also more spread out towards far away values (from the contribution from the normal distribution with standard deviation 10). The reason for using such a prior is that it is like a standard unit normal, but makes values far away from 0 more likely, allowing the model to explore a larger weight space. Run the code below to create a \"spike and slab\" distribution and plot its probability density function, compared with a standard unit normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i26mlCWooFW9"
      },
      "source": [
        "# Function to define the spike and slab distribution\n",
        "\n",
        "def spike_and_slab(event_shape, dtype):\n",
        "    distribution = tfd.Mixture(\n",
        "        cat=tfd.Categorical(probs=[0.5, 0.5]),\n",
        "        components=[\n",
        "            tfd.Independent(tfd.Normal(\n",
        "                loc=tf.zeros(event_shape, dtype=dtype), \n",
        "                scale=1.0*tf.ones(event_shape, dtype=dtype)),\n",
        "                            reinterpreted_batch_ndims=1),\n",
        "            tfd.Independent(tfd.Normal(\n",
        "                loc=tf.zeros(event_shape, dtype=dtype), \n",
        "                scale=10.0*tf.ones(event_shape, dtype=dtype)),\n",
        "                            reinterpreted_batch_ndims=1)],\n",
        "    name='spike_and_slab')\n",
        "    return distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFDxViE4oFW_"
      },
      "source": [
        "# Plot the spike and slab distribution pdf\n",
        "\n",
        "x_plot = np.linspace(-5, 5, 1000)[:, np.newaxis]\n",
        "plt.plot(x_plot, tfd.Normal(loc=0, scale=1).prob(x_plot).numpy(), label='unit normal', linestyle='--')\n",
        "plt.plot(x_plot, spike_and_slab(1, dtype=tf.float32).prob(x_plot).numpy(), label='spike and slab')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqlCcUdLoFXB"
      },
      "source": [
        "You should now complete the function below to create the prior distribution for the `DenseVariational` layer, using the spike and slab distribution above.\n",
        "\n",
        "* The function has the required signature for the `make_prior_fn` argument of the `DenseVariational` layer\n",
        "* The prior will have no trainable parameters\n",
        "* It should use the spike and slab distribution for both the kernel and the bias, setting the `dtype` according to the function argument\n",
        "* The distribution should have the correct event shape, according to the `kernel_size` and `bias_size` arguments\n",
        "* The function should return a callable, that returns the spike and slab distribution\n",
        "\n",
        "_Hints:_ \n",
        "* _Refer to the lecture video and/or coding tutorial to review the_ `DenseVariational` _layer arguments_\n",
        "* _Use the_ `Sequential` _API with a_ `DistributionLambda` _layer to create the callable that is returned by the function_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zByNMRX-oFXB"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following functions. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def get_prior(kernel_size, bias_size, dtype=None):\n",
        "    \"\"\"\n",
        "    This function should create the prior distribution, consisting of the \n",
        "    \"spike and slab\" distribution that is described above. \n",
        "    The distribution should be created using the kernel_size, bias_size and dtype\n",
        "    function arguments above.\n",
        "    The function should then return a callable, that returns the prior distribution.\n",
        "    \"\"\"\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = Sequential([\n",
        "        tfpl.DistributionLambda(\n",
        "            lambda t: spike_and_slab(event_shape=n, dtype=dtype)\n",
        "            #lambda t: tfd.MultivariateNormalDiag(loc=tf.zeros(n), scale_diag=tf.ones(n))\n",
        "        )\n",
        "    ])\n",
        "    return prior_model      \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPQ1S25woFXD"
      },
      "source": [
        "You'll use this function when you create the `DenseVariational` layer later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCy-J9eDoFXE"
      },
      "source": [
        "You should now complete the function below to create the variational posterior distribution for the `DenseVariational` layer. This distribution will be an independent Gaussian with trainable mean and standard deviation for each parameter in the layer.\n",
        "\n",
        "* The function has the required signature for the `make_posterior_fn` argument of the `DenseVariational` layer\n",
        "* The posterior will have 2 trainable variables for each layer parameter, one for the mean and one for the standard deviation\n",
        "* The distribution should have the correct event shape, according to the `kernel_size` and `bias_size` arguments\n",
        "* The function should return a callable, that returns the trainable independent Gaussian distribution\n",
        "\n",
        "_Hints:_ \n",
        "* _Refer to the lecture video and/or coding tutorial to review the_ `DenseVariational` _layer arguments_\n",
        "* _Use the_ `Sequential` _API with a_ `VariableLayer` and an `IndependentNormal` _layer to create the callable that is returned by the function_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zspebbazoFXE"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following functions. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def get_posterior(kernel_size, bias_size, dtype=None):\n",
        "    \"\"\"\n",
        "    This function should create the posterior distribution as specified above.\n",
        "    The distribution should be created using the kernel_size, bias_size and dtype\n",
        "    function arguments above.\n",
        "    The function should then return a callable, that returns the posterior distribution.\n",
        "    \"\"\"\n",
        "    n = kernel_size + bias_size\n",
        "    posterior_model = Sequential([\n",
        "        tfpl.VariableLayer(tfpl.IndependentNormal.params_size(n), dtype=dtype),\n",
        "        tfp.layers.IndependentNormal(n)\n",
        "    ])\n",
        "    return posterior_model         \n",
        "    "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qklM5W5oFXG"
      },
      "source": [
        "You should now use your `prior` and `posterior` functions to complete the function below to create the `DenseVariational` layer. \n",
        "\n",
        "* The function has `prior_fn`, `posterior_fn` and `kl_weight` arguments, to be used in the constructor of the `DenseVariational` layer\n",
        "* The layer should have the correct number of units in order to parameterize a `OneHotCategorical` layer with 10 categories\n",
        "* The `make_prior_fn`, `make_posterior_fn` and `kl_weight` arguments should be set with the corresponding function arguments\n",
        "* An exact KL-divergence is unavailable for this choice of prior and posterior, so the layer should not attempt to use an analytical expression for this\n",
        "* Your function should then return an instance of the `DenseVariational` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKVzcCPdoFXG"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following functions. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def get_dense_variational_layer(prior_fn, posterior_fn, kl_weight):\n",
        "    \"\"\"\n",
        "    This function should create an instance of a DenseVariational layer according \n",
        "    to the above specification. \n",
        "    The function takes the prior_fn, posterior_fn and kl_weight as arguments, which should \n",
        "    be used to define the layer.\n",
        "    Your function should then return the layer instance.\n",
        "    \"\"\"\n",
        "    dv_layer = tfpl.DenseVariational(\n",
        "        units=tfpl.OneHotCategorical.params_size(10),\n",
        "        make_posterior_fn=posterior_fn,\n",
        "        make_prior_fn=prior_fn,\n",
        "        kl_weight=kl_weight\n",
        "    )\n",
        "\n",
        "    return dv_layer        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhsF2-0boFXI"
      },
      "source": [
        "Now, you're ready to use the functions you defined to create the convolutional reparameterization and dense variational layers, and use them in your Bayesian convolutional neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vazOUoF8oFXI"
      },
      "source": [
        "# Create the layers\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "divergence_fn = lambda q, p, _ : tfd.kl_divergence(q, p) / x_train.shape[0]\n",
        "convolutional_reparameterization_layer = get_convolutional_reparameterization_layer(\n",
        "    input_shape=(28, 28, 1), divergence_fn=divergence_fn\n",
        ")\n",
        "dense_variational_layer = get_dense_variational_layer(\n",
        "    get_prior, get_posterior, kl_weight=1/x_train.shape[0]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnvIoaxyoFXL"
      },
      "source": [
        "# Build and compile the Bayesian CNN model\n",
        "\n",
        "bayesian_model = Sequential([\n",
        "    convolutional_reparameterization_layer,\n",
        "    MaxPooling2D(pool_size=(6, 6)),\n",
        "    Flatten(),\n",
        "    dense_variational_layer,\n",
        "    tfpl.OneHotCategorical(10, convert_to_tensor_fn=tfd.Distribution.mode)\n",
        "])\n",
        "bayesian_model.compile(loss=nll,\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'],\n",
        "              experimental_run_tf_function=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqH-uyc4oFXN"
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "bayesian_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Xy3mCsoFXP"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "bayesian_model.fit(x=x_train, y=y_train_oh, epochs=10, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy6zxkbBoFXR"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "print('Accuracy on MNIST test set: ',\n",
        "      str(bayesian_model.evaluate(x_test, y_test_oh, verbose=False)[1]))\n",
        "print('Accuracy on corrupted MNIST test set: ',\n",
        "      str(bayesian_model.evaluate(x_c_test, y_c_test_oh, verbose=False)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1DORpGooFXT"
      },
      "source": [
        "#### Analyse the model predictions\n",
        "\n",
        "Now that the model has trained, run the code below to create the same plots as before, starting with an analysis of the predicted probabilities for the same images. \n",
        "\n",
        "This model now has weight uncertainty, so running the forward pass multiple times will not generate the same estimated probabilities. For this reason, the estimated probabilities do not have single values. The plots are adjusted to show a 95% prediction interval for the model's estimated probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qtBvyfPMoFXU"
      },
      "source": [
        "# Prediction examples on MNIST\n",
        "\n",
        "for i in [0, 1577]:\n",
        "    analyse_model_prediction(x_test, y_test, bayesian_model, i, run_ensemble=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FflDcYZgoFXW"
      },
      "source": [
        "For the first image, the model assigns a probability of almost one for the 6 label. Furthermore, it is confident in this probability: this probability remains close to one for every sample from the posterior weight distribution (as seen by the horizontal green line having very small height, indicating a narrow prediction interval). This means that the epistemic uncertainty on this probability is very low. \n",
        "\n",
        "For the second image, the epistemic uncertainty on the probabilities is much larger, which indicates that the estimated probabilities may be unreliable. In this way, the model indicates whether estimates may be inaccurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rApHmbNoFXW"
      },
      "source": [
        "# Prediction examples on MNIST-C\n",
        "\n",
        "for i in [0, 3710]:\n",
        "    analyse_model_prediction(x_c_test, y_c_test, bayesian_model, i, run_ensemble=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0G-_2tyoFXZ"
      },
      "source": [
        "Even with the spatters, the Bayesian model is confident in predicting the correct label for the first image above. The model struggles with the second image, which is reflected in the range of probabilities output by the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjQcz_lroFXa"
      },
      "source": [
        "# Prediction examples from both datasets\n",
        "\n",
        "for i in [9241]:\n",
        "    analyse_model_prediction(x_test, y_test, bayesian_model, i, run_ensemble=True)\n",
        "    analyse_model_prediction(x_c_test, y_c_test, bayesian_model, i, run_ensemble=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLp9mTcpoFXc"
      },
      "source": [
        "Similar to before, the model struggles with the second number, as it is mostly covered up by the spatters. However, this time is clear to see the epistemic uncertainty in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH6wmfNAoFXc"
      },
      "source": [
        "#### Uncertainty quantification using entropy\n",
        "\n",
        "We also again plot the distribution of distribution entropy across the different test sets below. In these plots, no consideration has been made for the epistemic uncertainty, and the conclusions are broadly similar to those for the previous model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbz3LvrhoFXc"
      },
      "source": [
        "# Entropy plots for the MNIST dataset\n",
        "\n",
        "print('MNIST test set:')\n",
        "plot_entropy_distribution(bayesian_model, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhswTB_boFXf"
      },
      "source": [
        "# Entropy plots for the MNIST-C dataset\n",
        "\n",
        "print('Corrupted MNIST test set:')\n",
        "plot_entropy_distribution(bayesian_model, x_c_test, y_c_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jIdp-sKoFXh"
      },
      "source": [
        "Congratulations on completing this programming assignment! In the next week of the course we will look at the bijectors module and normalising flows."
      ]
    }
  ]
}